{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betamaan/Final_demo/blob/main/file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### requirements"
      ],
      "metadata": {
        "id": "zHMuSWqo_REO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain\n",
        "langchain-core\n",
        "\n",
        "langchain-openai\n",
        "openai\n",
        "\n",
        "langchain-anthropic\n",
        "\n",
        "langchain-google-genai\n",
        "google-generativeai\n",
        "\n",
        "langchain-huggingface\n",
        "transformers\n",
        "huggingface-hub\n",
        "\n",
        "python-dotenv\n",
        "\n",
        "numpy\n",
        "scikit-learn\n"
      ],
      "metadata": {
        "id": "04IxYvrkdmvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVZA3W4BkvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Agent 1 + 2"
      ],
      "metadata": {
        "id": "BBIkfopwkvGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "# === SETUP ===\n",
        "load_dotenv()\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "# === STAGE 1: Extract Citation Numbers ===\n",
        "print(\"\\n--- Stage 1: Extracting Citation Numbers ---\")\n",
        "\n",
        "pdf_path = \"/home/skumar/Langchain/file/paper1.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), (Smith et al., 2021), [Touvron et al., 2023], (Research, 2022)\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = citation_prompt | model | parser\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (citations)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "candidate_citations = sorted([\n",
        "    int(n) for n in all_citation_numbers\n",
        "    if str(n).isdigit() and 1 <= int(n) <= 300\n",
        "])\n",
        "\n",
        "verified_citations = set()\n",
        "for n in candidate_citations:\n",
        "    pattern_square = rf\"\\[{n}\\]\"\n",
        "    pattern_round = rf\"\\({n}\\)\"\n",
        "    for chunk in chunks:\n",
        "        if re.search(pattern_square, chunk.page_content) or re.search(pattern_round, chunk.page_content):\n",
        "            verified_citations.add(n)\n",
        "            break\n",
        "\n",
        "filtered_citation_numbers = sorted(verified_citations)\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/citation_num.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chun.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citation numbers found:\", len(filtered_citation_numbers))\n",
        "t_mid = time.time()\n",
        "print(f\"Time for Stage 1: {t_mid - t_start:.2f} seconds\")\n",
        "\n",
        "# === STAGE 2: Extract Author Names ===\n",
        "print(\"\\n--- Stage 2: Extracting Citation Authors ---\")\n",
        "\n",
        "# Reload saved data\n",
        "with open(\"results/citation_num.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/chun.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "author_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a section of an academic paper. Extract a list of properly formatted citation entries.\n",
        "\n",
        "Each entry must have:\n",
        "- The citation number\n",
        "- The full author list\n",
        "\n",
        "Format each citation like this:\n",
        "Citation No. 1: Kumar R., Sharma V.\n",
        "\n",
        "Return as JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"author\": \"Kumar R., Sharma V.\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Only include entries with author. Do not return anything else.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = author_prompt | model | parser\n",
        "final_citations = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (authors)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            author = entry.get(\"author\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                author and\n",
        "                \"not available\" not in author.lower() and\n",
        "                \"n/a\" not in author.lower()\n",
        "            ):\n",
        "                final_citations[int(cnum)] = f\"{author}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "with open(\"results/clear_citations.json\", \"w\") as f:\n",
        "    json.dump(final_citations, f, indent=2)\n",
        "\n",
        "print(\"\\nClean Citations Extracted:\")\n",
        "for cnum in sorted(final_citations):\n",
        "    print(f\"Citation {cnum}: {final_citations[cnum]}\")\n",
        "\n",
        "t_end = time.time()\n",
        "print(f\"\\nTime for Stage 2: {t_end - t_mid:.2f} seconds\")\n",
        "print(f\"Total Time: {t_end - t_start:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "TdC6S2yTku1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMymFnqXlxCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent No. 01\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper81.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), (Smith et al., 2021), [Touvron et al., 2023], (Research, 2022)\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = citation_prompt | model | parser\n",
        "\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "candidate_citations = sorted([\n",
        "    int(n) for n in all_citation_numbers\n",
        "    if str(n).isdigit() and 1 <= int(n) <= 300\n",
        "])\n",
        "\n",
        "verified_citations = set()\n",
        "\n",
        "for n in candidate_citations:\n",
        "    pattern_square = rf\"\\[{n}\\]\"\n",
        "    pattern_round = rf\"\\({n}\\)\"\n",
        "    found = False\n",
        "    for chunk in chunks:\n",
        "        if re.search(pattern_square, chunk.page_content) or re.search(pattern_round, chunk.page_content):\n",
        "            verified_citations.add(n)\n",
        "            break\n",
        "\n",
        "filtered_citation_numbers = sorted(verified_citations)\n",
        "\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citation numbers found:\", len(filtered_citation_numbers))\n",
        "\n",
        "t2 = time.time()\n",
        "print(f\" time needed for creating chunks and counting the citations is\", t1 - t1)\n",
        "# print(\"Saved to results/citation_numbers.json\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# AGENT 2\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load model\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Load previous results\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "# Prompt to extract author + year\n",
        "author_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a section of an academic paper. Extract a list of properly formatted citation entries.\n",
        "\n",
        "Each entry must have:\n",
        "- The citation number\n",
        "- The full author list\n",
        "\n",
        "Format each citation like this:\n",
        "Citation No. 1: Kumar R., Sharma V.\n",
        "\n",
        "Return as JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"author\": \"Kumar R., Sharma V.\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Only include entries with author . Do not anything else except this.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = author_prompt | model | parser\n",
        "\n",
        "final_citations = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            author = entry.get(\"author\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                author and\n",
        "                \"not available\" not in author.lower() and\n",
        "                \"n/a\" not in author.lower()\n",
        "            ):\n",
        "                final_citations[int(cnum)] = f\"{author}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "# Save final clean result\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/clean_citations.json\", \"w\") as f:\n",
        "    json.dump(final_citations, f, indent=2)\n",
        "\n",
        "print(\"\\n Clean Citations Extracted:\")\n",
        "for cnum in sorted(final_citations):\n",
        "    print(f\"Citation {cnum}: {final_citations[cnum]}\")\n",
        "\n",
        "t3 = time.time()\n",
        "print(F\" time for extracting the author names\", t2 - t3)\n",
        "\n"
      ],
      "metadata": {
        "id": "KyWRzO17mUM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UC5UethJmUJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0Octx-cmUF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0ODfiPtmT5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Agent 1 + 2 + 3\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ========== SETUP ==========\n",
        "load_dotenv()\n",
        "model = ChatOllama(model=\"mistral-large:123b\")\n",
        "parser = StrOutputParser()\n",
        "t_start = time.time()\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# ========== AGENT 1: Citation Numbers ==========\n",
        "print(\"\\n--- Agent 1: Extracting Citation Numbers ---\")\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper81.pdf\")\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), (Smith et al., 2021), [Touvron et al., 2023], (Research, 2022)\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "chain = citation_prompt | model | parser\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (citations)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "candidate_citations = sorted(n for n in all_citation_numbers if 1 <= int(n) <= 300)\n",
        "\n",
        "verified_citations = set()\n",
        "for n in candidate_citations:\n",
        "    pattern_square = rf\"\\[{n}\\]\"\n",
        "    pattern_round = rf\"\\({n}\\)\"\n",
        "    for chunk in chunks:\n",
        "        if re.search(pattern_square, chunk.page_content) or re.search(pattern_round, chunk.page_content):\n",
        "            verified_citations.add(n)\n",
        "            break\n",
        "\n",
        "filtered_citation_numbers = sorted(verified_citations)\n",
        "\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citations found:\", len(filtered_citation_numbers))\n",
        "t1 = time.time()\n",
        "print(f\"Agent 1 time: {t1 - t_start:.2f} sec\")\n",
        "\n",
        "# ========== AGENT 2: Citation Authors ==========\n",
        "print(\"\\n--- Agent 2: Extracting Citation Authors ---\")\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "author_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a section of an academic paper. Extract a list of properly formatted citation entries.\n",
        "\n",
        "Each entry must have:\n",
        "- The citation number\n",
        "- The full author list\n",
        "\n",
        "Format each citation like this:\n",
        "Citation No. 1: Kumar R., Sharma V.\n",
        "\n",
        "Return as JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"author\": \"Kumar R., Sharma V.\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Only include entries with author. Do not return anything else.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = author_prompt | model | parser\n",
        "final_citations = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (authors)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            author = entry.get(\"author\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                author and\n",
        "                \"not available\" not in author.lower() and\n",
        "                \"n/a\" not in author.lower()\n",
        "            ):\n",
        "                final_citations[int(cnum)] = author\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "with open(\"results/clean_citations.json\", \"w\") as f:\n",
        "    json.dump(final_citations, f, indent=2)\n",
        "\n",
        "print(\"\\nExtracted Citation Authors:\")\n",
        "for cnum in sorted(final_citations):\n",
        "    print(f\"Citation {cnum}: {final_citations[cnum]}\")\n",
        "\n",
        "t2 = time.time()\n",
        "print(f\"Agent 2 time: {t2 - t1:.2f} sec\")\n",
        "\n",
        "# ========== AGENT 3: Citation Years + Timeline ==========\n",
        "print(\"\\n--- Agent 3: Extracting Citation Years ---\")\n",
        "\n",
        "year_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a chunk of an academic research paper.\n",
        "\n",
        "Your task is to find the publication year for each in-text citation.\n",
        "Only return years if they are clearly associated with a citation.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"year\": \"2023\"}},\n",
        "  {{\"citation_no\": 2, \"year\": \"2020\"}}\n",
        "]\n",
        "\n",
        "Do not include citations without a year. Skip missing or unclear entries.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = year_prompt | model | parser\n",
        "citation_years = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (years)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            year = entry.get(\"year\")\n",
        "\n",
        "            if cnum in citation_numbers:\n",
        "                if year and re.match(r\"^(19|20)\\d{2}$\", str(year)):\n",
        "                    citation_years[int(cnum)] = year\n",
        "                else:\n",
        "                    # Add as unknown temporarily; we may overwrite later if found in other chunks\n",
        "                    citation_years[int(cnum)] = \"Unknown\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "for cnum in citation_numbers:\n",
        "    if int(cnum) not in citation_years:\n",
        "        citation_years[int(cnum)] = \"Unknown\"\n",
        "\n",
        "with open(\"results/years.json\", \"w\") as f:\n",
        "    json.dump(citation_years, f, indent=2)\n",
        "\n",
        "print(f\"\\nTotal citation years extracted: {len(citation_years)}\")\n",
        "\n",
        "# Timeline plotting\n",
        "def draw_basic_timeline(years_json_path, figSize=(15, 10)):\n",
        "    with open(years_json_path, 'r') as f:\n",
        "        years_map = json.load(f)\n",
        "\n",
        "    # citations_with_years = []\n",
        "    # for num_str, year_str in years_map.items():\n",
        "    #     try:\n",
        "    #         citations_with_years.append({\"number\": num_str, \"year\": int(year_str)})\n",
        "    #     except ValueError:\n",
        "    #         continue\n",
        "    citations_with_years = [\n",
        "        {\"number\": num_str, \"year\": int(year_str)}\n",
        "        for num_str, year_str in years_map.items()\n",
        "        if str(year_str).isdigit()\n",
        "    ]\n",
        "\n",
        "    citations_with_years.sort(key=lambda x: x['year'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figSize)\n",
        "    min_year = min(c['year'] for c in citations_with_years) - 1\n",
        "    max_year = max(c['year'] for c in citations_with_years) + 1\n",
        "\n",
        "    ax.hlines(0, min_year, max_year, color='gray', linestyle='-', linewidth=1.5)\n",
        "    ax.set_xticks(range(min_year, max_year + 1))\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_xlim(min_year, max_year)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.spines[['left', 'right', 'top']].set_visible(False)\n",
        "    ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "    y_offset_factor = 0.1\n",
        "    y_positions = {}\n",
        "\n",
        "    for citation in citations_with_years:\n",
        "        year = citation['year']\n",
        "        label = f\"#{citation['number']}\"\n",
        "        current_y_offset = y_positions.get(year, 0)\n",
        "        y_pos = y_offset_factor * ((current_y_offset + 1) * (-1 if current_y_offset % 2 else 1))\n",
        "        y_positions[year] = current_y_offset + 1\n",
        "\n",
        "        ax.plot(year, 0, 'o', color='darkblue', markersize=6)\n",
        "        ax.plot([year, year], [0, y_pos], color='skyblue', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            xy=(year, y_pos),\n",
        "            xytext=(year, y_pos + (0.02 if y_pos > 0 else -0.02)),\n",
        "            fontsize=9,\n",
        "            ha='center',\n",
        "            va='bottom' if y_pos > 0 else 'top',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
        "            arrowprops=dict(arrowstyle=\"-\", color='gray', linewidth=0.5)\n",
        "        )\n",
        "\n",
        "    plt.title(\"Citation Timeline\", fontsize=14, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    try:\n",
        "        output_path = Path(years_json_path).with_name(\"Basic_Citations_Timeline.jpeg\")\n",
        "        fig.savefig(output_path, format=\"jpeg\", bbox_inches=\"tight\", dpi=300)\n",
        "        print(f\"Timeline image saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save image: {e}\")\n",
        "\n",
        "draw_basic_timeline(\"results/years.json\")\n",
        "t3 = time.time()\n",
        "print(f\"Agent 3 time: {t3 - t2:.2f} sec\")\n",
        "print(f\"\\n All agents completed in {t3 - t_start:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "Plw0aW1Klw-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiH01gq8mCgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Timeline"
      ],
      "metadata": {
        "id": "EV6Krt1AmCdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGENT 3\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/clean_citations.json\") as f:\n",
        "    author_map = json.load(f)\n",
        "\n",
        "year_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a chunk of an academic research paper.\n",
        "\n",
        "Your task is to find the publication year for each in-text citation.\n",
        "Only return years if they are clearly associated with a citation.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"year\": \"2023\"}},\n",
        "  {{\"citation_no\": 2, \"year\": \"2020\"}}\n",
        "]\n",
        "\n",
        "Do not include citations without a year. Skip missing or unclear entries.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = year_prompt | model | parser\n",
        "\n",
        "citation_years = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            year = entry.get(\"year\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                year and\n",
        "                re.match(r\"^(19|20)\\d{2}$\", str(year))\n",
        "            ):\n",
        "                citation_years[int(cnum)] = year\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/years.json\", \"w\") as f:\n",
        "    json.dump(citation_years, f, indent=2)\n",
        "\n",
        "# print(\"\\n Verified Citation Years:\")\n",
        "# for cnum in sorted(citation_years):\n",
        "#     print(f\"Citation {cnum}: {citation_years[cnum]}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def draw_basic_timeline(years_json_path, figSize=(15, 10)):\n",
        "    with open(years_json_path, 'r', encoding='utf-8') as f:\n",
        "        years_map = json.load(f)\n",
        "\n",
        "    citations_with_years = []\n",
        "\n",
        "    for num_str, year_str in years_map.items():\n",
        "        try:\n",
        "            year = int(year_str)\n",
        "            citations_with_years.append({\n",
        "                \"number\": num_str,\n",
        "                \"year\": year\n",
        "            })\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    citations_with_years.sort(key=lambda x: x['year'])\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=figSize)\n",
        "    min_year = min(c['year'] for c in citations_with_years) - 1\n",
        "    max_year = max(c['year'] for c in citations_with_years) + 1\n",
        "\n",
        "    ax.hlines(0, min_year, max_year, color='gray', linestyle='-', linewidth=1.5)\n",
        "\n",
        "    ax.set_xlabel(\"Year\", fontsize=12)\n",
        "    ax.set_xticks(range(min_year, max_year + 1))\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_xlim(min_year, max_year)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.spines[['left', 'right', 'top']].set_visible(False)\n",
        "    ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "    y_offset_factor = 0.1\n",
        "    y_positions = {}\n",
        "\n",
        "    for citation in citations_with_years:\n",
        "        year = citation['year']\n",
        "        label = f\"#{citation['number']}\"\n",
        "        current_y_offset = y_positions.get(year, 0)\n",
        "\n",
        "        if year % 2 == 0:\n",
        "            y_pos = y_offset_factor * ((current_y_offset  + 0.5 // 2) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "        else:\n",
        "            y_pos = y_offset_factor * ((current_y_offset - 0.5 // 2 + 1) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "\n",
        "        ax.plot(year, 0, 'o', color='darkblue', markersize=6)\n",
        "        ax.plot([year, year], [0, y_pos], color='skyblue', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            xy=(year, y_pos),\n",
        "            xytext=(year, y_pos + (0.02 if y_pos > 0 else -0.02)),\n",
        "            fontsize=9,\n",
        "            ha='center',\n",
        "            va='bottom' if y_pos > 0 else 'top',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
        "            arrowprops=dict(arrowstyle=\"-\", color='gray', linewidth=0.5)\n",
        "        )\n",
        "\n",
        "    plt.title(\"Citation Timeline (Using Only years.json)\", fontsize=14, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    try:\n",
        "        output_path = Path(years_json_path).with_name(\"Basic_Citations_Timeline.jpeg\")\n",
        "        fig.savefig(output_path, format=\"jpeg\", bbox_inches=\"tight\", dpi=300)\n",
        "        print(f\"Timeline image saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save image: {e}\")\n",
        "\n",
        "# ========== MAIN ==========\n",
        "if __name__ == \"__main__\":\n",
        "    draw_basic_timeline(\"/home/skumar/Langchain/.venv/results/years.json\")\n"
      ],
      "metadata": {
        "id": "SpOek4PhmCbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaxtUiK6mCZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Summary + Cluster + Visualize\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/results/clean_citations.json\") as f:\n",
        "    author_map = json.load(f)\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Given this chunk of a research paper, generate a unique 2-line summary for each citation.\n",
        "\n",
        "Format as a JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"summary\": \"This study explored ...\"}},\n",
        "  {{\"citation_no\": 2, \"summary\": \"Authors evaluated ...\"}}\n",
        "]\n",
        "\n",
        "Do not repeat the same summary.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = summary_prompt | model | parser\n",
        "\n",
        "citation_summaries = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            summary = entry.get(\"summary\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                summary and\n",
        "                \"not available\" not in summary.lower() and\n",
        "                \"n/a\" not in summary.lower() and\n",
        "                cnum not in citation_summaries\n",
        "            ):\n",
        "                citation_summaries[int(cnum)] = summary.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "with open(\"results/summaries.json\", \"w\") as f:\n",
        "    json.dump(citation_summaries, f, indent=2)\n",
        "\n",
        "print(\"\\n Citation Summaries Extracted:\")\n",
        "for cnum in sorted(citation_summaries):\n",
        "    print(f\"Citation {cnum}: {citation_summaries[cnum]}\")\n",
        "\n",
        "\n",
        "# %pip install matplotlib\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from collections import defaultdict\n",
        "\n",
        "with open(\"/home/skumar/Langchain/.venv/results/summaries.json\", \"r\") as f:\n",
        "    citation_data = json.load(f)\n",
        "\n",
        "citation_ids = list(citation_data.keys())\n",
        "citation_texts = list(citation_data.values())\n",
        "\n",
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # use \"llama3\" if you want full LLM embedding\n",
        "embeddings = embedding_model.embed_documents(citation_texts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of k values to test\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(embeddings)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Optional: Plot the elbow curve to visualize\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia (sum of squared distances)')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"results/elbow_curve.png\")  # Save plot instead of showing it (since this is often run headless)\n",
        "plt.close()\n",
        "\n",
        "# Automatically find \"elbow point\" using slope change (simple heuristic)\n",
        "def find_elbow_point(inertias):\n",
        "    diffs = [inertias[i] - inertias[i+1] for i in range(len(inertias)-1)]\n",
        "    slopes = [diffs[i] - diffs[i+1] for i in range(len(diffs)-1)]\n",
        "    return slopes.index(max(slopes)) + 2  # +2 because index offset\n",
        "\n",
        "optimal_k = find_elbow_point(inertias)\n",
        "print(f\"Optimal k (by elbow method): {optimal_k}\")\n",
        "\n",
        "\n",
        "k = optimal_k\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "clusters_raw = defaultdict(list)\n",
        "for idx, label in enumerate(labels):\n",
        "    clusters_raw[label].append((citation_ids[idx], citation_texts[idx]))\n",
        "\n",
        "cluster_summaries = []\n",
        "for i, entries in clusters_raw.items():\n",
        "    cluster_text = \"\\n\".join([f\"[{cid}] {text}\" for cid, text in entries])\n",
        "    cluster_summaries.append((f\"Cluster {i+1}\", cluster_text))\n",
        "\n",
        "model = ChatOllama(model=\"mistral:latest\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "cluster_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant. You are given a group of research summaries, each prefixed by a citation number like [22].\n",
        "\n",
        "Your task is to analyze this group and organize it into labeled subgroups based on theme, technique, or topic.\n",
        "Return the result as a JSON object with structure like:\n",
        "\n",
        "{{\n",
        "  \"Main Theme of This Cluster\": {{\n",
        "    \"Subgroup A\": [22, 23],\n",
        "    \"Subgroup B\": [24, 25]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Only use citation numbers in the output. Do not include summaries or explanations.\n",
        "\n",
        "Cluster label: {label}\n",
        "Entries:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"label\", \"text\"]\n",
        ")\n",
        "\n",
        "final_clusters = {}\n",
        "\n",
        "for label, cluster_text in cluster_summaries:\n",
        "    try:\n",
        "        result = (cluster_prompt | model | parser).invoke({\"label\": label, \"text\": cluster_text})\n",
        "        parsed = json.loads(result[result.find(\"{\"):])\n",
        "        final_clusters[label] = parsed\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {label}: {e}\")\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\", \"w\") as f:\n",
        "    json.dump(final_clusters, f, indent=2)\n",
        "print(\"Hybrid clustering complete. Output saved to results/hybrid_llm_embedding_clusters.json\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def build_hierarchy(data):\n",
        "    children = []\n",
        "    for cluster, structure in data.items():\n",
        "        theme = list(structure.keys())[0]\n",
        "        cluster_node = {\"name\": cluster, \"children\": []}\n",
        "        for subgroup, citations in structure[theme].items():\n",
        "            subgroup_node = {\"name\": subgroup, \"children\": [{\"name\": f\"Citation {c}\"} for c in citations]}\n",
        "            cluster_node[\"children\"].append(subgroup_node)\n",
        "        children.append(cluster_node)\n",
        "    return {\"name\": \"Root\", \"children\": children}\n",
        "\n",
        "hierarchy = build_hierarchy(data)\n",
        "\n",
        "with open(\"results/d3_citation_tree.json\", \"w\") as f:\n",
        "    json.dump(hierarchy, f, indent=2)\n",
        "\n",
        "print(\"D3-compatible JSON saved to results/d3_citation_tree.json\")\n"
      ],
      "metadata": {
        "id": "hGrnZw4umCWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}